{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "790fe347-8563-4f92-8c5c-173bf49e8008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./flant5/lib/python3.9/site-packages (2.1.1)\n",
      "Requirement already satisfied: torchvision in ./flant5/lib/python3.9/site-packages (0.16.1)\n",
      "Requirement already satisfied: torchaudio in ./flant5/lib/python3.9/site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in ./flant5/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy in ./flant5/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: fsspec in ./flant5/lib/python3.9/site-packages (from torch) (2023.12.1)\n",
      "Requirement already satisfied: jinja2 in ./flant5/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: networkx in ./flant5/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions in ./flant5/lib/python3.9/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: requests in ./flant5/lib/python3.9/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./flant5/lib/python3.9/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: numpy in ./flant5/lib/python3.9/site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./flant5/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./flant5/lib/python3.9/site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./flant5/lib/python3.9/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./flant5/lib/python3.9/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./flant5/lib/python3.9/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./flant5/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Users/arush/workspace/646proj/LaMP/data/flant5/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: transformers in ./flant5/lib/python3.9/site-packages (4.35.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex!=2019.12.17 in ./flant5/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./flant5/lib/python3.9/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./flant5/lib/python3.9/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./flant5/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./flant5/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./flant5/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./flant5/lib/python3.9/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: filelock in ./flant5/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: requests in ./flant5/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./flant5/lib/python3.9/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./flant5/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./flant5/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./flant5/lib/python3.9/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./flant5/lib/python3.9/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./flant5/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./flant5/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Users/arush/workspace/646proj/LaMP/data/flant5/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "!pip3 install transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "import requests\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a12cfb7e-3283-45ae-b7e8-a7d8278c3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/arush/workspace/646proj/LaMP/data/sampled_rating_20.json\", \"r\") as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5856e725-0990-47aa-a9f7-1d13e741f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('msmarco-distilbert-base-dot-prod-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e986334-e27b-4d8b-b6c8-abaa72933800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1fd5d134d540b1b7294263c92ebc7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f07c8dde-effb-4a1d-ba69-2298a2831218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4385405b2a5049a6b79c047a0d4861b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/flan-t5-xl\"\n",
    "summarizer = pipeline(\"summarization\", model = model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62b45d03-7a40-4a68-baa1-c65f5d9c92be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████████████████████▋                                                                                                                                                       | 3/20 [00:25<02:04,  7.31s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (989 > 512). Running this sequence through the model will result in indexing errors\n",
      " 30%|█████████████████████████████████████████████████████                                                                                                                            | 6/20 [09:54<25:18, 108.43s/it]Your max_length is set to 200, but your input_length is only 98. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [19:57<00:00, 59.85s/it]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "golds = []\n",
    "\n",
    "for user in tqdm(json_data):\n",
    "    i += 1\n",
    "    corpus = []\n",
    "    scores = []\n",
    "    profile = user['profile']\n",
    "    query = user['input']\n",
    "\n",
    "    input_prompt = \"Your task is to answer the query based on user's relevant history interactions.\\nHere’s the query: {}\\n\".format(query)\n",
    "    past_interactions = []\n",
    "    ratings_with_reviews = {str(i): [] for i in range(1, 6)} \n",
    "\n",
    "    for doc in profile:\n",
    "        corpus.append(doc['text'])\n",
    "        scores.append(doc['score'])\n",
    "\n",
    "    corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5, score_function=util.dot_score)\n",
    "    hits = hits[0]\n",
    "    predictions = {}\n",
    "\n",
    "    for hit in hits:\n",
    "        score = str(scores[hit['corpus_id']])  # Convert to string\n",
    "        ratings_with_reviews[score].append(corpus[hit['corpus_id']])\n",
    "\n",
    "    word_limit = 512\n",
    "    for key, value in ratings_with_reviews.items():\n",
    "        all_reviews = \"\"\n",
    "        if value:\n",
    "            sentences_str = \" \".join(value)\n",
    "            words_so_far = len(all_reviews.split())\n",
    "            words_in_current = len(sentences_str.split())\n",
    "    \n",
    "            if words_so_far + words_in_current <= word_limit:\n",
    "                all_reviews += sentences_str + \" \"\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            final_result = \" \".join(all_reviews.split()[:word_limit])\n",
    "            input_prompt += \"Product rating given by the user for the following review: '{}' is '{}'\".format(summarizer(final_result)[0]['summary_text'], key) + \"\\n\"\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\")\n",
    "    outputs = llm_model.generate(**inputs)\n",
    "    predictions[\"output\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    predictions[\"id\"] = user[\"id\"]\n",
    "    golds.append(predictions)\n",
    "\n",
    "final_json = {}\n",
    "final_json[\"task\"] = \"LaMP_3\"\n",
    "final_json[\"golds\"] = golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c0d8600-42e1-41ce-a6cf-38fa90b24a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pred_summarized_profile_product_rating.json\", \"w\") as f:\n",
    "    json.dump(final_json, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
